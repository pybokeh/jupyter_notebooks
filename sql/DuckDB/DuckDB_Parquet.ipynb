{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e70f6a-552e-4f4f-acf1-c1a1ce23a596",
   "metadata": {},
   "source": [
    "# Using DuckDB's SQL as an alternative workflow for large CSV / Parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c567ec29-d86e-4e21-b46d-4b99440897ba",
   "metadata": {},
   "source": [
    "For large CSV files, we may look to use Dask or PySpark even, when loading the CSV to a database is not possible.  But, there is an alternative SQL-based workflow using DuckDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976bcffe-89ae-4190-b97c-97735658d31b",
   "metadata": {},
   "source": [
    "### Let's start with Dask workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75eb778-afc2-4f8c-bf46-4e53579af6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920bc125-4acc-4693-b5f9-26246260505c",
   "metadata": {},
   "source": [
    "#### For best practice, it is usually best to define data type, instead of having dask infer/guess the data type (HINT: type inference is often wrong and leads to poor performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc746a6-37f7-4af0-ae7a-773204e7cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = {\n",
    "    'SUPP_NO': str,\n",
    "    'ASN_NO': str,\n",
    "    'ACTUAL_DT': str,\n",
    "    'ACTUAL_TIME': str,\n",
    "    'DC_SEG': 'category',\n",
    "    'TRLR_NO': str,\n",
    "    'TRLR_ARR_DT': str,\n",
    "    'TRLR_ARR_TIME': str,\n",
    "    'ORDERED_QTY': np.int32,   # Use lowest, memory-consuming \"bit-ness\" (Are we going to order more than 64K parts?  Negative values?)\n",
    "    'ASN_PART_QTY': np.int32,         # ditto\n",
    "    'PART_UNLD_QTY': np.int32,        # ditto\n",
    "    'TOTAL_ASN_PART_QTY': np.int32,   # ditto\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915b4311-dfa4-4c87-a3ec-199bcaf87d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv('shipping_receiving_all_plants.csv', dtype=col_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a445dc9a-ac71-41e4-88bf-3c6e49b33f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3015bd08-d56a-42bb-ac6c-352a9b575407",
   "metadata": {},
   "source": [
    "#### Let's time how long it would take to sum a column using the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cfd365-10c0-4f7f-aa31-2e09d4484ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df['ORDERED_QTY'].sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d64991b-4c7e-4cdc-82a8-6427b9ad69ab",
   "metadata": {},
   "source": [
    "#### Thanks to Dask's parallelization, we didn't blow out our local machine's memory, but it still took on average almost 50 seconds.  Can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cdc392-220c-4024-be36-0a08b0ee5ed4",
   "metadata": {},
   "source": [
    "## Querying the parquet format using Dask instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53e2ac-c842-4ab6-a039-151051bcae54",
   "metadata": {},
   "source": [
    "As before, we should define the data type or schema for the columns.  Parquet's underlying technology is Apache Arrow, so we need to use pyarrow's data types instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d333b0b-af34-44a9-9de8-9641686d40ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9df4e8-9a6d-4ca0-8f9d-29c08f1f007f",
   "metadata": {},
   "source": [
    "#### Converting CSV to parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05046fa6-89f9-454b-b1b9-922c0654e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_schema = {\n",
    "    'SUPP_NO': pa.string(),\n",
    "    'ASN_NO': pa.string(),\n",
    "    'ACTUAL_DT': pa.string(),\n",
    "    'ACTUAL_TIME': pa.string(),\n",
    "    'DC_SEG': pa.string(),\n",
    "    'TRLR_NO': pa.string(),\n",
    "    'TRLR_ARR_DT': pa.string(),\n",
    "    'TRLR_ARR_TIME': pa.string(),\n",
    "    'ORDERED_QTY': pa.int32(),\n",
    "    'ASN_PART_QTY': pa.int32(),\n",
    "    'PART_UNLD_QTY': pa.int32(),\n",
    "    'TOTAL_ASN_PART_QTY': pa.int32(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854134bc-235b-4d98-a493-72ad7a95cf54",
   "metadata": {},
   "source": [
    "#### Now save Dask dataframe as a parquet file.  UPDATE: Later found out that repartioning to a single partition, boosted performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb80d07d-c914-41df-b71d-886c74a3da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df.repartition(npartitions=1)\n",
    "df.to_parquet('s_r_all_plants.parquet', write_index=False, schema=col_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f4974-333a-401c-b969-b723efdb8934",
   "metadata": {},
   "source": [
    "The conversion resulted in a folder called `s_r_all_plants.parquet` folder containing a single partition parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d265060-d406-4387-af9c-018c0f8eddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir s_r_all_plants.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3613c3-777d-409a-ae14-d424a97ff6dc",
   "metadata": {},
   "source": [
    "With Dask, we don't need to read all the individual parquet files, just reference the folder containing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfea8a8b-89d7-49aa-8ce9-f265895fb4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dfp = dd.read_parquet('s_r_all_plants.parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b76f7d-eb8b-4826-95a6-4a6f9fe8ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1dd373-eff4-4c7e-9d6e-56f5b8b64e03",
   "metadata": {},
   "source": [
    "#### Computing the sum on parquet file took less than a second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018756d7-5fd8-4e62-8e86-9f02acaae852",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "dfp['ORDERED_QTY'].sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51522eb-4e81-4910-8b33-bd770b976d52",
   "metadata": {},
   "source": [
    "But, what if you don't want to use dataframe API?  But a SQL workflow instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df317660-77f1-4e23-8d36-f4e4788df5cc",
   "metadata": {},
   "source": [
    "## SQL Workflow using DuckDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c71f2-f342-4483-81ea-ec57762358c7",
   "metadata": {},
   "source": [
    "Starting with version 0.2.7, DuckDB now supports querying parquet files!  Just `pip install duckdb`, then you're good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d4224-f1e5-4b9e-b176-461beea4a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ffd365-ac85-4679-a24e-aa2a8a2ff8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c52d67b-d5c1-4091-8429-db4caac29104",
   "metadata": {},
   "source": [
    "#### I prefer to use context manager (using `with` statement) when connecting to databases.  But, DuckDB does not yet support context management.  So I have to implement a custom class myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a71300-8d4f-40c5-aee7-6952eb620325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuckDBConn:\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        pass\n",
    "    def __enter__(self):\n",
    "        \"\"\"\n",
    "        Open the database connection\n",
    "        \"\"\"\n",
    "        self.conn = duckdb.connect()\n",
    "        return self.conn\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"\n",
    "        Close the connection\n",
    "        \"\"\"\n",
    "        self.conn.close()\n",
    "        if exc_val:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51969270-105f-42cd-a7b9-37a1aa535a4b",
   "metadata": {},
   "source": [
    "DuckDB supports wildcard \"globbing\" with the asterisk symbol to read in multiple parquet files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1882127-f9a4-4c82-b711-f2e71edd7e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "with DuckDBConn() as con:\n",
    "    df_sr = con.execute(\"\"\"\n",
    "        SELECT sum(ORDERED_QTY) as sum_ordered\n",
    "        FROM 's_r_all_plants.parquet/*.parquet'\n",
    "    \"\"\").df()\n",
    "    \n",
    "df_sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5299a88a-f833-48c2-afb2-c4935b477d73",
   "metadata": {},
   "source": [
    "DuckDB was just as fast as Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6074e2ac-9b2d-4375-88c8-c5201d42b3cd",
   "metadata": {},
   "source": [
    "Let's try to convert the parquet files into a single parquet file to see if there could be performance improvements.  But the problem is, there doesn't seem to be good documentation to determine optimal `row_group_size`.  Through trial and error, I settled on 200K rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e62eb9f-15e4-47ed-9139-f99d65021159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eb2e9d-5d6b-49de-a4eb-c742bdedb869",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(pq.ParquetDataset('s_r_all_plants.parquet/').read(), 'shipping_receiving.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb80e7a-ad9b-4ba3-ae47-f585e566a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c7fc64-f750-4824-9b4f-89fea54b3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "with DuckDBConn() as con:\n",
    "    df_sr = con.execute(\"\"\"\n",
    "       SELECT sum(ORDERED_QTY) as sum_ordered\n",
    "       FROM 'shipping_receiving.parquet'\n",
    "       \"\"\").df()\n",
    "    \n",
    "df_sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123aada0-77d7-4cfd-9e5e-dcd8a98a1cfa",
   "metadata": {},
   "source": [
    "With DuckDB, we got some speed boost with a single file, parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ec9799-9e00-475e-8a59-1cfca5055993",
   "metadata": {},
   "source": [
    "#### Let's see if there is a difference with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd56d61-4fc0-4c89-ab00-7b71f07b08f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = dd.read_parquet('shipping_receiving.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe64da-450e-4118-b84d-187ffbaa0a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dfp['ORDERED_QTY'].sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f750e6d-0c98-4b21-9b81-921c3d0b528f",
   "metadata": {},
   "source": [
    "With Dask reading a single file, parquet file, it did not get much, if any speed boost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
