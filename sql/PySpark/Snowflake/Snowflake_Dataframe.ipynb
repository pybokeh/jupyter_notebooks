{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e379f8e-0528-414d-a473-ce79b881c415",
   "metadata": {},
   "source": [
    "## Snowflake Query as PySpark Dataframe and PySpark Dataframe as Snowflake Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0e038-1522-4d92-95ac-45e7d2b0b478",
   "metadata": {},
   "source": [
    "**NOTE:** If you are running local PySpark on Ubuntu WSL, you will need to install additional dependencies in order for your Windows host browser to be launched from WSL, which has to happen when using Snowflake's browser authentication.\n",
    "\n",
    "sudo apt-get update<br>\r\n",
    "sudo apt-get install xdg-util\n",
    "\n",
    "and also install wslu: https://wslutiliti.es/wslu/install.html#ubuntu.  Then create environment variable:\n",
    "\n",
    "`export BROWSER=wslview`\n",
    "\n",
    "**SOURCE:** https://superuser.com/questions/1262977/open-browser-in-host-system-from-windows-subsystem-for-linuxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e026fc-0a48-4810-96db-e579b07444d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import configparser\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3ec3a3c-fe51-4650-95cf-82e4afdcf72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = os.getenv(\"CONFIG_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b3dbe97-9653-4d0f-ab5f-9824bcf10f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "try:\n",
    "    config.read(config_file)\n",
    "except ConfigFileNotFound:\n",
    "    print(\"config.ini file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433f0b9c-d73f-4023-bcb6-9e164d531a16",
   "metadata": {},
   "source": [
    "JDBC driver and Snowflake Spark Connector can be downloaded [here](https://search.maven.org/search?q=g:net.snowflake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c3c25e5-3e3a-4aa9-b375-56d79d9bdf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_jdbc_driver = config['snowflake']['jdbc_driver_path']\n",
    "sf_connector = config['snowflake']['spark_driver_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c35bf46d-1ff2-4ac9-81cf-e6410f928056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/i33859/jdbc_drivers/snowflake/snowflake-jdbc-3.13.30.jar'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf_jdbc_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d15cf5fd-42e0-4529-8374-32b26955308f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/i33859/jdbc_drivers/snowflake/spark-snowflake_2.12-2.12.0-spark_3.4.jar'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf_connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b848e687-7f93-4827-ab94-306ba25c5199",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_account = config['snowflake']['account']\n",
    "sf_user = config['snowflake']['username']\n",
    "sf_database = config['snowflake']['database']\n",
    "sf_schema = config['snowflake']['schema']\n",
    "sf_role = config['snowflake']['role']\n",
    "sf_warehouse = config['snowflake']['warehouse']\n",
    "sf_authenticator = config['snowflake']['authenticator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "775e8b88-2ea7-42a8-88b8-0cbb1635a250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/22 19:55:12 WARN Utils: Your hostname, VA-rveOJ44nPxI1 resolves to a loopback address: 127.0.1.1; using 192.168.56.1 instead (on interface eth1)\n",
      "23/06/22 19:55:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/06/22 19:55:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"Snowflake_JDBC\")\n",
    "    .config(\"spark.jars\", f\"{sf_jdbc_driver},{sf_connector}\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7281dd0-734a-4807-af85-318759568703",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e05df36a-405c-46a0-b658-2cc2503c590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowflake connection parameters\n",
    "sfparams = {\n",
    "    \"sfURL\" : f\"{sf_account}.snowflakecomputing.com\",\n",
    "    \"sfUser\" : sf_user,\n",
    "    \"sfPassword\" : \"your_password\",  # Not applicable when using externalbrowser authenticator\n",
    "    \"sfDatabase\" : sf_database,\n",
    "    \"sfSchema\" : sf_schema,\n",
    "    \"sfRole\" : sf_role,\n",
    "    \"sfWarehouse\" : sf_warehouse,\n",
    "    \"sfAuthenticator\" : sf_authenticator\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8976c5ef-6eab-4314-93ec-55e110773b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT CURRENT_DATE as my_date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fd443ce-0593-49e8-be2c-fb3a125b8fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n"
     ]
    }
   ],
   "source": [
    "#run custom query\n",
    "df = (\n",
    "    spark.read.format(SNOWFLAKE_SOURCE_NAME)\n",
    "    .options(**sfparams)\n",
    "    .option(\"query\", query)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05fea43a-8c4d-4448-b82c-e561b1440e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   MY_DATE|\n",
      "+----------+\n",
      "|2023-06-22|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc2fa32-d7f0-4c76-89c5-70ac88da1b10",
   "metadata": {},
   "source": [
    "#### Dataframe to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd1cbd5-cb8a-47ef-856c-a6a85bc94353",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .select(\"my_date\").write.format(SNOWFLAKE_SOURCE_NAME)\n",
    " .options(**sfparams)\n",
    " .option(\"dbtable\", \"my_table\")\n",
    " # https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameWriter.mode.html#pyspark.sql.DataFrameWriter.mode\n",
    " .mode(\"overwrite\")\n",
    " .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abacdbf-4526-4cf0-8c73-a22acda66e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c196350-e7e0-48e4-973d-a4a8184885e9",
   "metadata": {},
   "source": [
    "#### Using Context Manager (with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a50ad3-2e4c-4afd-8543-6a7022b21265",
   "metadata": {},
   "outputs": [],
   "source": [
    "with (SparkSession.builder.master(\"local[*]\").appName(\"Snowflake_JDBC\").config(\"spark.jars\", f\"{sf_jdbc_driver},{sf_connecctor}\").getOrCreate()) as spark:\n",
    "    query = \"SELECT CURRENT_DATE as my_date\"\n",
    "    jdbcDF = (\n",
    "        spark.read.format(SNOWFLAKE_SOURCE_NAME)\n",
    "        .options(**sfparams)\n",
    "        .option(\"query\", query)\n",
    "        .load()\n",
    "    )\n",
    "    jdbcDF.show()\n",
    "    \n",
    "    (jdbcDF\n",
    "     .select(\"my_date\").write.format(SNOWFLAKE_SOURCE_NAME)\n",
    "     .options(**sfparams)\n",
    "     .option(\"dbtable\", \"my_table\")\n",
    "     # https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameWriter.mode.html#pyspark.sql.DataFrameWriter.mode\n",
    "     .mode(\"overwrite\")\n",
    "     .save()\n",
    "    )\n",
    "    print(\"Completed saving dataframe as Snowflake table\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3.10 (pyspark_dev v3.4.0)",
   "language": "python",
   "name": "pyspark_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
