{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ce543a-658e-4c09-b837-d81102e35f8c",
   "metadata": {},
   "source": [
    "# Validating Snowflake Tables Using PySpark and Great Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f8c65-be13-43f7-b0cb-39dff0fe5950",
   "metadata": {},
   "source": [
    "**Example validations:**\n",
    "  - Ensure columns are present in a table\n",
    "  - Ensure values in a column are within a specified range or min/max, etc\n",
    "  - Ensure that a table column has at least one value from a set of values\n",
    "  - many more validations planned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e73f09-25a6-4f36-950a-a101027a695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import configparser\n",
    "import os\n",
    "import pyspark\n",
    "from great_expectations.dataset import SparkDFDataset\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from typing import Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daacd160-cc4a-46e4-a2ab-feb791e84221",
   "metadata": {},
   "source": [
    "## Custom helper functions to create pyspark session and create pyspark dataframe from Snowflake query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc19643-efaa-482c-b62c-78cf22cb594e",
   "metadata": {},
   "source": [
    "**NOTE:** These examples assume that we have stored Snowflake credentials in a `config.ini` text file whose location is specified in the `CONFIG_PATH` environment variable (using Windows: `set CONFIG_PATH=path/to/config.ini` or using Linux/MacOS: `export CONFIG_PATH=paht/to/config.ini`).  Furthermore, it is assumed you have access to a PySpark environment that is capable of connecting to your Snowflake environment: has the proper JDBC driver and connector jar files registered which are also referenced in the `config.ini` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43743ce0-a460-4d1e-b5da-ee32fbde9597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_snowflake_session():\n",
    "    config_file = os.getenv(\"CONFIG_PATH\")\n",
    "    \n",
    "    config = configparser.ConfigParser()\n",
    "    try:\n",
    "        config.read(config_file)\n",
    "    except ConfigFileNotFound:\n",
    "        print(\"config.ini file not found\")\n",
    "\n",
    "    sf_jdbc_driver = config['snowflake']['jdbc_driver_path']\n",
    "    sf_connector = config['snowflake']['spark_driver_path']\n",
    "\n",
    "    return (\n",
    "        SparkSession.builder.master(\"local[*]\")\n",
    "        .appName(\"Snowflake_JDBC\")\n",
    "        .config(\"spark.jars\", f\"{sf_jdbc_driver},{sf_connector}\")\n",
    "        .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3cfe3eb-d83d-4dd9-82f3-3eb56105d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_snowflake_dataframe_from_sql(session: SparkSession, schema: str, sql: str) -> DataFrame:\n",
    "    config_file = os.getenv(\"CONFIG_PATH\")\n",
    "    \n",
    "    config = configparser.ConfigParser()\n",
    "    try:\n",
    "        config.read(config_file)\n",
    "    except ConfigFileNotFound:\n",
    "        print(\"config.ini file not found\")\n",
    "\n",
    "    sf_account = config['snowflake']['account']\n",
    "    sf_user = config['snowflake']['username']\n",
    "    sf_database = config['snowflake']['database']\n",
    "    # sf_schema = config['snowflake']['schema']\n",
    "    sf_role = config['snowflake']['role']\n",
    "    sf_warehouse = config['snowflake']['warehouse']\n",
    "    sf_authenticator = config['snowflake']['authenticator']\n",
    "\n",
    "    # Snowflake connection parameters\n",
    "    sfparams = {\n",
    "        \"sfURL\" : f\"{sf_account}.snowflakecomputing.com\",\n",
    "        \"sfUser\" : sf_user,\n",
    "        \"sfPassword\" : \"your_password\",  # Not applicable when using externalbrowser authenticator\n",
    "        \"sfDatabase\" : sf_database,\n",
    "        \"sfSchema\" : schema,\n",
    "        \"sfRole\" : sf_role,\n",
    "        \"sfWarehouse\" : sf_warehouse,\n",
    "        \"sfAuthenticator\" : sf_authenticator\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        spark.read.format('net.snowflake.spark.snowflake')\n",
    "        .options(**sfparams)\n",
    "        .option(\"query\", sql)\n",
    "        .load()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2765c1e-f88c-4017-8ad1-2793a03c8b80",
   "metadata": {},
   "source": [
    "## Custom tests to validate NHTSA tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c66c551a-6e88-40e4-bcb9-3f385103a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mandatory_columns_existence(df: SparkDFDataset, columns: Iterator):\n",
    "    for column in columns:\n",
    "        try:\n",
    "            assert df.expect_column_to_exist(column).success, f\"ERROR: Mandatory column '{column}' does not exist\"\n",
    "            print(f\"Column '{column}' exists : PASSED\")\n",
    "        except AssertionError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b14fdee9-40c8-4ab0-813f-bd7d0a9ba12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_year_min_max(df: SparkDFDataset, year_column: str, start_year: int, end_year: int):\n",
    "    try:\n",
    "        assert df.expect_column_values_to_be_between(year_column, start_year, end_year).success, f\"ERROR: Failed min/max YEAR test\"\n",
    "        print(f\"PASSED min/max YEAR test\")\n",
    "    except AssertionError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41642d01-e209-4e88-80fe-fdcd854ae532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_make_names(df: SparkDFDataset, make_name_column: str, make_names=Iterator):\n",
    "    try:\n",
    "        assert df.expect_column_values_to_be_in_set(make_name_column, value_set=make_names).success, f\"ERROR: Failed make names test\"\n",
    "        print(f\"PASSED make names test\")\n",
    "    except AssertionError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23064b6e-c774-414d-93cf-f69db8232759",
   "metadata": {},
   "source": [
    "## `main()` routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b795746d-011c-4147-831a-4bc2c174739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/27 08:56:17 WARN Utils: Your hostname, VA-rveOJ44nPxI1 resolves to a loopback address: 127.0.1.1; using 192.168.56.1 instead (on interface eth1)\n",
      "23/06/27 08:56:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/06/27 08:56:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n",
      "###########################################################################\n",
      "Performing validation tests...\n",
      "Column 'MAKE_ID' exists : PASSED\n",
      "Column 'MAKE_NAME' exists : PASSED\n",
      "Column 'MODEL_ID' exists : PASSED\n",
      "Column 'MODEL_NAME' exists : PASSED\n",
      "Column 'VEHICLETYPEID' exists : PASSED\n",
      "Column 'VEHICLETYPENAME' exists : PASSED\n",
      "Column 'YEAR' exists : PASSED\n",
      "Column 'CREATED_DATE' exists : PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED min/max YEAR test\n",
      "PASSED make names test\n",
      "Finished validation tests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/27 08:56:50 WARN SparkConnectorContext$: Finish cancelling all queries for local-1687870579371\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spark = get_spark_snowflake_session()\n",
    "\n",
    "    sql = \"\"\"\n",
    "        SELECT * from nhtsa.model_names where vehicletypename != 'Motorcycle'\n",
    "    \"\"\"\n",
    "\n",
    "    sdf = get_spark_snowflake_dataframe_from_sql(session=spark, schema='nhtsa', sql=sql)\n",
    "\n",
    "    test_df = SparkDFDataset(sdf)\n",
    "\n",
    "    #  Run Great Expectations test suite\n",
    "    MANDATORY_COLUMNS = [\n",
    "        'MAKE_ID',\n",
    "        'MAKE_NAME',\n",
    "        'MODEL_ID',\n",
    "        'MODEL_NAME',\n",
    "        'VEHICLETYPEID',\n",
    "        'VEHICLETYPENAME',\n",
    "        'YEAR',\n",
    "        'CREATED_DATE'\n",
    "    ]\n",
    "\n",
    "    MAKE_NAMES = [\n",
    "        'ACURA',\n",
    "        'ALFA ROMEO',\n",
    "        'ASTON MARTIN',\n",
    "        'AUDI',\n",
    "        'BENTLEY',\n",
    "        'BMW',\n",
    "        'BUGATTI',\n",
    "        'BUICK',\n",
    "        'CADILLAC',\n",
    "        'CHEVROLET',\n",
    "        'CHRYSLER',\n",
    "        'DODGE',\n",
    "        'FERRARI',\n",
    "        'FIAT',\n",
    "        'FORD',\n",
    "        'GENESIS',\n",
    "        'GMC',\n",
    "        'HONDA',\n",
    "        'HUMMER',\n",
    "        'HYUNDAI',\n",
    "        'INFINITI',\n",
    "        'ISUZU',\n",
    "        'JAGUAR',\n",
    "        'JEEP',\n",
    "        'KIA',\n",
    "        'LAMBORGHINI',\n",
    "        'LANCIA',\n",
    "        'LAND ROVER',\n",
    "        'LEXUS',\n",
    "        'LINCOLN',\n",
    "        'LOTUS',\n",
    "        'LUCID',\n",
    "        'MASERATI',\n",
    "        'MAZDA',\n",
    "        'MCLAREN',\n",
    "        'MERCEDES-BENZ',\n",
    "        'MERCURY',\n",
    "        'MINI',\n",
    "        'MITSUBISHI',\n",
    "        'NISSAN',\n",
    "        'OPEL',\n",
    "        'PLYMOUTH',\n",
    "        'POLESTAR',\n",
    "        'PONTIAC',\n",
    "        'PORSCHE',\n",
    "        'RAM',\n",
    "        'RIVIAN',\n",
    "        'ROLLS ROYCE',\n",
    "        'SAAB',\n",
    "        'SATURN',\n",
    "        'SMART',\n",
    "        'SUBARU',\n",
    "        'SUZUKI',\n",
    "        'TESLA',\n",
    "        'TOYOTA',\n",
    "        'VOLKSWAGEN',\n",
    "        'VOLVO'\n",
    "    ]\n",
    "\n",
    "    print(\"###########################################################################\")\n",
    "    print(\"Performing validation tests...\")\n",
    "    test_mandatory_columns_existence(test_df, columns=MANDATORY_COLUMNS)\n",
    "    test_year_min_max(test_df, year_column='YEAR', start_year=2009, end_year=2023)\n",
    "    test_make_names(test_df, make_name_column='MAKE_NAME', make_names=MAKE_NAMES)\n",
    "\n",
    "    print(\"Finished validation tests\")\n",
    "\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3.4 (pyspark_dev)",
   "language": "python",
   "name": "pyspark_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
