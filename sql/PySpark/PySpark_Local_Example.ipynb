{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbafa2e2-1394-45ba-bd7a-84fc5c676995",
   "metadata": {},
   "source": [
    "# It is recommended that you install local PySpark on OSs other than Windows OS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b9eca-6e63-4474-a82e-032bc2f6a832",
   "metadata": {},
   "source": [
    "## or use Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a957cd8-e0e8-4dc9-8db6-408b8fe874f2",
   "metadata": {},
   "source": [
    "## Installing \"Local\" PySpark on Windows 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eddb094-e340-4c0a-9e3a-23ad14b1d975",
   "metadata": {},
   "source": [
    "With PySpark on Windows, there is a chance that you won't be able to save or export your dataframe onto your local file system if we are not able to successfully trick Windows into thinking Hadoop was installed (that is the purpose of step 8 below). But you should be able to at least read a local text or CSV file.  The background on this is that on Windows, having native Hadoop is NOT optional, it is requried.  In contrast with other OSs, it is optional or not required.  There is currently not a good way to determine which winutils.exe version to obtain.  See this Github [issue](https://github.com/cdarlint/winutils/issues/20) for details.  This will take trial and error - you will have to try using various different hadoop/winutils.exe versions until you no longer get the error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b4c310-2ac1-487a-b9ac-505cb567a3e6",
   "metadata": {},
   "source": [
    "1. Install Java 1.8 from Sun Java [site](https://www.java.com/download/ie_manual.jsp). Include path to java.exe in your PATH environment variable.\n",
    "2. Install Python - download binaries at python.org\n",
    "3. Create pyspark_dev virtual environment: `python -m venv pyspark_dev`\n",
    "4. Change directory into `pyspark_dev` folder: `cd pyspark_dev`.  Then activate \"pyspark_dev\" environment with `Scripts/activate.bat`\n",
    "5. Update pip and then install necessary packages: `python -m pip install -U pip`, then `pip install wheel`, then `pip install pyspark ipykernel`\n",
    "6. Install kernel: `python -m ipykernel install --user --name pyspark_dev --display-name \"Python (pyspark_dev)\"`\n",
    "7. Set environment variables: `PYSPARK_PYTHON=[path_to_python.exe]` and `SPARK_HOME=[path_to_site_packages/pyspark folder]`\n",
    "8. This step will take trial and error.  Go to https://github.com/cdarlint/winutils and download the entire repo.  Then save contents of different version's `bin` folder into your local `hadoop/bin` folder until you can write to local file system without getting an error. See this Github [issue](https://github.com/cdarlint/winutils/issues/20) for background on how others were able to resolve their issue.\n",
    "9. `set HADOOP_HOME=[path_to_hadoop_folder]` and append `HADOOP_HOME\\bin` to PATH: `set PATH=%PATH%;%HADOOP_HOME%\\bin`\n",
    "10. De-activate your pyspark_dev virtual environment, then activate your python virutal environment that has jupyterlab installed.\n",
    "11. Confirm you have pyspark_dev installed as a kernel by issuing the following command: `jupyter kernelspec list`  If you see it, then launch jupyterlab via `jupyter lab`\n",
    "12. Choose your PySpark kernel that you defined in Step 5 when opening a new jupyterlab notebook\n",
    "\n",
    "See this article https://phoenixnap.com/kb/install-spark-on-windows-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c863be59-4529-421d-8c9a-9d397d682f16",
   "metadata": {},
   "source": [
    "## Installing \"Local\" PySpark on Ubuntu Linux WSL via pip in virtual environment, NOT system level installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c2b965-a2d6-4ed1-9e75-dfcae5e1f702",
   "metadata": {},
   "source": [
    "1. Install Java 1.8 `sudo apt-get update` then `sudo apt-get install openjdk-8-jdk` and set JAVA_HOME environment variable, as an example: `export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64` or execute `whereis java` to find the location.  Do NOT set JAVA_HOME to the `bin` directory, just the root Java directory.\n",
    "2. Create \"pyspark_dev\" virtual environment: `python3 -m venv pyspark_dev`\n",
    "3. Activate \"pyspark_dev\" environment, then: `python -m pip install -U pip`, then `pip install wheel`, then `PYSPARK_HADOOP_VERSION=3 pip install pyspark pandas ipykernel`\n",
    "4. Install kernel: `python -m ipykernel install --user --name pyspark_dev --display-name \"Python (pyspark_dev)\"`\n",
    "5. Add 2 environment variables (SPARK_HOME and PYSPARK_PYTHON), as an example: `export SPARK_HOME=/home/pybokeh/envs/pyspark_dev/lib/python3.10/site-packages/pyspark` and `export PYSPARK_PYTHON=/home/pybokeh/envs/pyspark_dev/bin/python`\n",
    "6. Append `SPARK_HOME/bin` and `SPARK_HOME/sbin` to your PATH, as an example: `export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin`\n",
    "7. `source ~/.bashrc` or `source ~/.profile`\n",
    "8. Issue the \"pyspark\" command to check if everything was installed correctly\n",
    "9. De-activate your pyspark_dev virtual environment, then activate your python virutal environment that has jupyterlab installed.\n",
    "10. Confirm you have pyspark_dev installed as a kernel by issuing the following command: `jupyter kernelspec list`  If you see it, then launch jupyterlab via `jupyter lab`\n",
    "11. Choose your PySpark kernel that you defined in Step 4 when opening a new jupyterlab notebook\n",
    "\n",
    "**NOTE:** When starting your PySpark session, you will see warnings about `SPARK_LOCAL_IP` or loopback address or native-hadoop not found.  You can safely ignore them for the purpose of running local PySpark environment.  For learning purposes, we don't need to actually install hadoop and we did not install hadoop in the steps above.\n",
    "\n",
    "Official installation [instructions](https://spark.apache.org/docs/latest/api/python/getting_started/install.html) from spark documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4309f-a368-48c0-90cf-94ebe5208c32",
   "metadata": {},
   "source": [
    "## To see if our local PySpark is working correctly, confirm we can read a CSV file and also save a dataframe as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a179242-6552-4f0f-81e0-3e3b21a56f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"local_pyspark\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c991e9ac-bf9e-42e2-89ae-bdad142eaf5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/cars.csv', header=True, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bca9f97-40b0-4d66-9f6a-235b7125e70f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
      "|                 Car| MPG|Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|\n",
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
      "|Chevrolet Chevell...|18.0|        8|       307.0|     130.0| 3504.|        12.0|   70|    US|\n",
      "|   Buick Skylark 320|15.0|        8|       350.0|     165.0| 3693.|        11.5|   70|    US|\n",
      "|  Plymouth Satellite|18.0|        8|       318.0|     150.0| 3436.|        11.0|   70|    US|\n",
      "|       AMC Rebel SST|16.0|        8|       304.0|     150.0| 3433.|        12.0|   70|    US|\n",
      "|         Ford Torino|17.0|        8|       302.0|     140.0| 3449.|        10.5|   70|    US|\n",
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6634e7-f9a6-459e-b46a-cbf8975d8693",
   "metadata": {},
   "source": [
    "#### If using Windows and get an error trying to save dataframe to csv file, you can try installing/using local PySpark on Linux/MacOS instead or repeat step 8 with a different hadoop / winutils.exe version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "605ef608-3986-4cef-9502-f60697d707e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.coalesce(1).write.mode('overwrite').option(\"header\", \"true\").csv(\"data/cars_single_partition.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3.11 (pyspark_dev)",
   "language": "python",
   "name": "pyspark_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
